import numpy as np
import pandas as pd

from dataloader import load_data, display_face
from pca import *
# from ex1_multi_class import logistic_regression

EMOTIONS = ['h', 'ht', 'm', 's', 'f', 'a', 'd', 'n']
PCA_NUMBER_OF_COMPONENT = 16
LEARNING_RATE = .015
EPOCHS = 20
DECISION_THRESHOLD = 0.5
TRAINING_PERCENTAGE = 0.6
VALIDATION_PERCENTAGE = 0.2
TEST_PERCENTAGE = 0.2
NUMBER_OF_ALL_EMOTIONS = 6
NUMBER_OF_SUBJECTS = 10
NUMBER_OF_RUNS = 5

RELAVENT_EMOTIONS = ['a', 'd', 'f', 'h', 'm', 's']
UNWANTED_EMOTION = ['ht', 'n']

EPOCHS_TO_INCLUDE_STD = [5, 10, 15, 20]
IMAGE_SCALE = 255

def simplify_labels(filename):
    labels = find_between(filename, '_', '.')
    return labels[:-1]


def find_between(s, first, last):
    start = s.index(first) + len(first)
    end = s.index(last, start)
    return s[start:end]


def import_data():
    images, labels = load_data()
    for i in range(len(labels)):
        labels[i] = simplify_labels(labels[i])

    return np.array(images), np.array(labels)


def arrange_data_set_for_all_emotions():
    images, labels = import_data()

    subjects_indices = get_shuffled_subject_indicies_for_all_emotions()
    relevant_images, relevant_labels = filter_relevant_subject(images, labels, subjects_indices)
    relevant_labels = encode_onehot_labels(relevant_labels)

    # Break down the data into training, testing and validation
    training_ratio = int(subjects_indices.size * TRAINING_PERCENTAGE)
    validation_ratio = int(subjects_indices.size * (TRAINING_PERCENTAGE + VALIDATION_PERCENTAGE))

    relevant_labels = relevant_labels.astype(np.int)

    training_images = relevant_images[:training_ratio]
    training_labels = relevant_labels[:training_ratio]

    validation_images = relevant_images[training_ratio: validation_ratio]
    validation_labels = relevant_labels[training_ratio: validation_ratio]

    test_images = relevant_images[validation_ratio:]
    test_labels = relevant_labels[validation_ratio:]

    return training_images, validation_images, test_images, training_labels, validation_labels, test_labels


def encode_onehot_labels(relevant_labels):
    data_table = pd.DataFrame({'emotion': relevant_labels})
    data_table = data_table[~data_table['emotion'].isin(UNWANTED_EMOTION)]
    data_table = pd.concat((data_table, pd.get_dummies(data_table.emotion)), 1)
    data_table = data_table.drop(columns=['emotion'])
    relevant_labels = data_table.values
    return relevant_labels


def filter_relevant_subject(images, labels, subjects_indices):
    indices_of_wanted_emotions = np.where(np.logical_and(labels != UNWANTED_EMOTION[0], labels != UNWANTED_EMOTION[1]))[0]
    relevant_images = images[indices_of_wanted_emotions][subjects_indices]
    relevant_labels = labels[indices_of_wanted_emotions][subjects_indices]
    return relevant_images, relevant_labels


def get_shuffled_subject_indicies_for_all_emotions():
    subjects_indices = np.arange(NUMBER_OF_SUBJECTS)
    np.random.shuffle(subjects_indices)
    subjects_indices = np.repeat(subjects_indices, NUMBER_OF_ALL_EMOTIONS)
    subjects_indices *= NUMBER_OF_ALL_EMOTIONS
    for j in range(NUMBER_OF_SUBJECTS):
        for i in range(NUMBER_OF_ALL_EMOTIONS):
            subjects_indices[i+(j*NUMBER_OF_ALL_EMOTIONS)] += i

    return subjects_indices


def batch_gradient_decent(samples, labels, validation_samples, validation_labels, is_stochastic = True):
    current_weights = np.zeros((PCA_NUMBER_OF_COMPONENT + 1, NUMBER_OF_ALL_EMOTIONS))
    first_train_loss = get_softmax_weights_loss(samples, labels, current_weights)
    best_validation_loss = get_softmax_weights_loss(validation_samples, validation_labels, current_weights)
    best_weights = current_weights

    number_of_training_samples = samples.shape[0]
    validation_errors = [best_validation_loss]
    training_errors = [first_train_loss]

    for t in range(EPOCHS):
        logistic_results = softmax_regression(samples, current_weights)

        leaning_order_indices = np.arange(number_of_training_samples)
        if(is_stochastic):
            np.random.shuffle(leaning_order_indices)

        loss_gradient = (labels - logistic_results)[leaning_order_indices].T @ samples[leaning_order_indices]
        current_weights = current_weights + LEARNING_RATE * loss_gradient.T

        current_validation_loss = get_softmax_weights_loss(validation_samples, validation_labels, current_weights)
        current_train_loss = get_softmax_weights_loss(samples, labels, current_weights)
        validation_errors.append(current_validation_loss)
        training_errors.append(current_train_loss)
        if best_validation_loss > current_validation_loss:
            best_validation_loss = current_validation_loss
            best_weights = current_weights

    return best_weights, np.array(training_errors), np.array(validation_errors)


def softmax_regression(samples, weights):
    a = samples @ weights
    e_a = np.exp(a.T)
    probabilities = e_a / e_a.sum(axis=0)
    return probabilities.T


def get_softmax_weights_loss(samples, labels, weights):
    predictions = softmax_regression(samples, weights)
    N = predictions.shape[0]
    loss = -np.sum(labels * np.log(predictions)) / N
    return loss


def run_pca_on_samples(pca, images):
    number_of_images = images.shape[0]
    pca_images = np.empty(((number_of_images, 1, PCA_NUMBER_OF_COMPONENT)))

    for i in range(number_of_images):
        pca_images[i] = pca.transform(images[i])

    return pca_images


def add_bias_coordinate(pca_images):
    number_of_images = pca_images.shape[0]
    pca_images = pca_images.reshape((number_of_images, PCA_NUMBER_OF_COMPONENT))
    bias_coordinates = np.ones((number_of_images, 1))
    return np.hstack((pca_images, bias_coordinates))


def regression_loss(labels, prediction):
    return -np.mean(labels * np.log(prediction) + (1 - labels) * np.log(1 - prediction))


def train_data(principle_component_number):
    training_images, validation_images, test_images, \
    training_labels, validation_labels, test_labels = arrange_data_set_for_all_emotions()

    test_pca_images, training_pca_images, validation_pca_images = run_pca_on_data(principle_component_number,
                                                                                  test_images, training_images,
                                                                                  validation_images)

    test_pca_images, training_pca_images, validation_pca_images = add_bias_column_to_data(test_pca_images,
                                                                                          training_pca_images,
                                                                                          validation_pca_images)

    weights, training_errors, validation_errors = batch_gradient_decent(training_pca_images, training_labels,
                                                                        validation_pca_images, validation_labels, False)

    accuracy = get_softmax_weights_accuracy(test_pca_images, test_labels, weights)
    confusion_matrix = get_softmax_confusion_matrix(test_pca_images, test_labels, weights)


    return training_errors, validation_errors, accuracy, confusion_matrix


def compare_batch_to_stocastic_training(principle_component_number):
    training_images, validation_images, test_images, \
    training_labels, validation_labels, test_labels = arrange_data_set_for_all_emotions()

    test_pca_images, training_pca_images, validation_pca_images = run_pca_on_data(principle_component_number,
                                                                                  test_images, training_images,
                                                                                  validation_images)

    test_pca_images, training_pca_images, validation_pca_images = add_bias_column_to_data(test_pca_images,
                                                                                          training_pca_images,
                                                                                          validation_pca_images)

    weights, normal_training_errors, validation_errors = batch_gradient_decent(training_pca_images, training_labels,
                                                                        validation_pca_images, validation_labels, False)

    weights, stochastic_training_errors, validation_errors = batch_gradient_decent(training_pca_images, training_labels,
                                                                        validation_pca_images, validation_labels, True)

    plt.plot(normal_training_errors, label="batch training")
    plt.plot(stochastic_training_errors, label="stochastic training")
    plt.xlabel("number of epochs")
    plt.ylabel("loss")
    plt.title("Batch vs. Stochastic Errors")
    plt.legend()
    plt.show()


def add_bias_column_to_data(test_pca_images, training_pca_images, validation_pca_images):
    training_pca_images = add_bias_coordinate(training_pca_images)
    validation_pca_images = add_bias_coordinate(validation_pca_images)
    test_pca_images = add_bias_coordinate(test_pca_images)
    return test_pca_images, training_pca_images, validation_pca_images


def run_pca_on_data(principle_component_number, test_images, training_images, validation_images):
    pca = PCA(principle_component_number)
    pca.fit(training_images)
    training_pca_images = run_pca_on_samples(pca, training_images)
    validation_pca_images = run_pca_on_samples(pca, validation_images)
    test_pca_images = run_pca_on_samples(pca, test_images)
    return test_pca_images, training_pca_images, validation_pca_images


def get_softmax_weights_accuracy(pca_images, labels, weights):
    predictions = softmax_regression(pca_images, weights)
    decisions = (predictions == predictions.max(axis=1)[:,None]).astype(np.int)

    diff = labels + decisions
    AGREEMENT_VALUE = 2
    diff[diff < AGREEMENT_VALUE] = 0
    diff[diff == AGREEMENT_VALUE] = 1

    number_of_images = pca_images.shape[0]
    return np.sum(diff) / number_of_images


def get_softmax_confusion_matrix(pca_images, labels, weights):
    predictions = softmax_regression(pca_images, weights)
    decisions = (predictions == predictions.max(axis=1)[:, None]).astype(np.int)

    number_of_labels = labels.shape[0]
    confusion_matrix = np.zeros((NUMBER_OF_ALL_EMOTIONS, NUMBER_OF_ALL_EMOTIONS))

    for i in range(number_of_labels):
        actual = np.where(labels[i] == 1)[0]
        predicted = np.where(decisions[i] == 1)[0]
        confusion_matrix[actual, predicted] += 1

    return confusion_matrix


def train_n_times_for_k_principle_components(n, k):
    all_training_errors = np.zeros((n, EPOCHS + 1))
    all_validation_errors = np.zeros((n, EPOCHS + 1))
    accuracies = np.zeros(n)
    sum_of_confusion_matrices = np.zeros((NUMBER_OF_ALL_EMOTIONS, NUMBER_OF_ALL_EMOTIONS))

    for i in range(n):
        training_errors, validation_errors, accuracy, confusion_matrix = train_data(k)
        all_training_errors[i] = training_errors
        all_validation_errors[i] = validation_errors
        accuracies[i] = accuracy
        avg_training_errors = np.average(all_training_errors, axis=0)
        avg_validation_errors = np.average(all_validation_errors, axis=0)
        sum_of_confusion_matrices += confusion_matrix

    std_training_errors = np.std(all_training_errors, axis=0)
    std_validation_errors = np.std(all_validation_errors, axis=0)
    plt.plot(avg_training_errors, label="training error")
    plt.plot(avg_validation_errors, label="validation error")
    plt.xlabel("number of epochs")
    plt.ylabel("loss")
    plt.title("Average error as a function of the number of epochs with " + str(k) + " principle components")
    plt.legend()
    plt.show()

    print()
    print("STDs:")
    for i in EPOCHS_TO_INCLUDE_STD:
        print("Epoch #" + str(i) + " std for training errors is " + str(std_training_errors[i]))
        print("Epoch #" + str(i) + " std for validation errors is " + str(std_validation_errors[i]))

    print()
    print("The average accuracy for all the five runs is " + str(np.average(accuracies)) +
          " with std: " + str(np.std(accuracies)))

    print()
    average_confusion_matrix = sum_of_confusion_matrices / (n * 2)
    print("The average confusion matrix for all the five runs is:")
    print(str(average_confusion_matrix))

    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(average_confusion_matrix)
    plt.title("The average confusion matrix for all the five runs")
    fig.colorbar(cax)
    ax.set_xticklabels([''] + RELAVENT_EMOTIONS)
    ax.set_yticklabels([''] + RELAVENT_EMOTIONS)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()


def visualize_weights(principle_component_number):
    training_images, validation_images, test_images, \
    training_labels, validation_labels, test_labels = arrange_data_set_for_all_emotions()

    pca = PCA(principle_component_number)
    pca.fit(training_images)

    training_pca_images = run_pca_on_samples(pca, training_images)
    validation_pca_images = run_pca_on_samples(pca, validation_images)
    test_pca_images = run_pca_on_samples(pca, test_images)

    test_pca_images, training_pca_images, validation_pca_images = add_bias_column_to_data(test_pca_images,
                                                                                          training_pca_images,
                                                                                          validation_pca_images)

    weights, training_errors, validation_errors = batch_gradient_decent(training_pca_images, training_labels,
                                                                        validation_pca_images, validation_labels,
                                                                        False)

    # removing the bias coordinates
    first_pca_image = training_pca_images[0,:-1]
    fig = plt.figure(figsize = (8,8))

    for emotion in RELAVENT_EMOTIONS:
        emotion_index = RELAVENT_EMOTIONS.index(emotion)
        weight = weights[:-1, emotion_index]
        emphasized_pca_image = np.inner(weight, first_pca_image)
        recovered_image = pca.inverse_transform(emphasized_pca_image)
        recovered_image = IMAGE_SCALE * (recovered_image - np.min(recovered_image)) / np.ptp(recovered_image).astype(int)

        sub = fig.add_subplot(3, 3, emotion_index + 1)
        plt.title(emotion)
        plt.axis('off')
        sub.imshow(recovered_image, cmap='gray')

    fig.suptitle('Image emphasized with the different emotions weights', fontsize=20)
    fig.show()